{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS2S6s6_H8oG"
      },
      "outputs": [],
      "source": [
        "#importing numpy and name it as npy\n",
        "import numpy as npy\n",
        "\n",
        "# importing tensor flow library and importing dataset for the project\n",
        "import tensorflow_datasets as ten_ds\n",
        "import tensorflow as ten_fw\n",
        "\n",
        "ten_ds.disable_progress_bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP_n3COJK1yf",
        "outputId": "b3e4cc3c-d658-4d65-f21e-771b38881515"
      },
      "outputs": [],
      "source": [
        "# downloading information and dataset for project and dataset have label\n",
        "# the dataset is based on the reviews of movies colled by imdb\n",
        "dataset, info = ten_ds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "tr_data_set, tst_data_set = dataset['train'], dataset['test']\n",
        "\n",
        "tr_data_set.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45d66nfUsmJA",
        "outputId": "0f6f4c4c-6d99-40cf-f0e1-dd35d4752096"
      },
      "outputs": [],
      "source": [
        "#These lines of code iterate over the dataset items using a for loop and increment a counter for each element to count the number of rows in the train and test datasets.\n",
        "num_rows_train = 0\n",
        "for _ in tr_data_set:\n",
        "    num_rows_train += 1\n",
        "\n",
        "num_rows_test = 0\n",
        "for _ in tst_data_set:\n",
        "    num_rows_test += 1\n",
        "\n",
        "print(\"Number of rows in train dataset:\", num_rows_train)\n",
        "print(\"Number of rows in test dataset:\", num_rows_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8yk-6xAPZUq",
        "outputId": "01ea4f06-22ba-4d04-e7f8-073ecd6ae313"
      },
      "outputs": [],
      "source": [
        "# Print top 5 rows of train dataset\n",
        "for text, label in tr_data_set.take(5):\n",
        "    print(text.numpy(), label.numpy())\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-hpnjunQc9Q",
        "outputId": "7ef85cd3-4894-4dad-a3cc-55cc68b9b997"
      },
      "outputs": [],
      "source": [
        "# Print top 5 rows of test dataset\n",
        "for text, label in tst_data_set.take(5):\n",
        "    print(text.numpy(), label.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jwpBzaQfUy",
        "outputId": "9e030a13-b44e-4996-d4a8-133675c1b305"
      },
      "outputs": [],
      "source": [
        "# it shows the code filters the rows in the train and test datasets that have empty text, and then prints the number of those rows.\n",
        "empty_rows_train = tr_data_set.filter(lambda x, y: ten_fw.strings.strip(x) == \"\")\n",
        "empty_rows_test = tst_data_set.filter(lambda x, y: ten_fw.strings.strip(x) == \"\")\n",
        "\n",
        "print(\"Number of empty rows in train dataset:\", len(list(empty_rows_train)))\n",
        "print(\"Number of empty rows in test dataset:\", len(list(empty_rows_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrfPSggAK4RQ",
        "outputId": "1b1ef7f4-5a67-412a-fb9e-146f862884bb"
      },
      "outputs": [],
      "source": [
        "#printing the random one example from the dataset\n",
        "for example, label in tr_data_set.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tHaB7BLLV-_"
      },
      "outputs": [],
      "source": [
        "# setting the size of buffer and batch size for the executation\n",
        "# BUFFER_SIZE is a hyperparameter that determines the number of elements from the dataset that the tf.data.Dataset object should prefetch at each iteration. It is used to speed up the training process by overlapping the preprocessing of data with the training of the model.\n",
        "#Another hyperparameter, BATCH_SIZE, controls the number of samples that will be used in a single training cycle. The average gradient derived from the samples in one batch is used to update the model's parameters. The batch size is a compromise between the accuracy of the model updates and computing efficiency. Although a higher batch size might result in more stable updates, it might also call for more memory and processing power.\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE7A9MyWLYzH"
      },
      "outputs": [],
      "source": [
        "# randomized the dataset so that model doesnot learn similar kind of pattern\n",
        "# randomized in model is necessary so that model does not learn simliar kind of pattern.\n",
        "tr_data_set = tr_data_set.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(ten_fw.data.AUTOTUNE)\n",
        "tst_data_set = tst_data_set.batch(BATCH_SIZE).prefetch(ten_fw.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ28KUsmLbnG",
        "outputId": "593b8c1a-4868-4792-99b1-94d68ed5f3b7"
      },
      "outputs": [],
      "source": [
        "# now printing the texts and labels in the training dataset from the randomized dataset \n",
        "for example, label in tr_data_set.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqEf5qwqRJU1",
        "outputId": "5764df12-ba58-42e9-b20c-60b86dc9751b"
      },
      "outputs": [],
      "source": [
        "# now printing the texts and labels in the training dataset from the randomized dataset \n",
        "for example, label in tst_data_set.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnHQout0Ldy2"
      },
      "outputs": [],
      "source": [
        "# it means that the thousands most frequent words will be keep in the vocabulary at the time of tokenizations\n",
        "# It alters the text input so that it may be represented numerically and supplied into a machine learning model.\n",
        "VOCAB_SIZE = 1000\n",
        "encoder = ten_fw.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(tr_data_set.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAIz9hzXLg-O",
        "outputId": "857375c2-2bc4-4bfb-b0b5-b389e1c6c241"
      },
      "outputs": [],
      "source": [
        "# retreving the first twenty words from the vocab which will be used for the training\n",
        "vocab = npy.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7kG6wwjLq2y",
        "outputId": "65d21a20-6148-44a7-b4d3-0bff0c4fc897"
      },
      "outputs": [],
      "source": [
        "#\tencoder(example) applies the text vectorization to the example and returns a tensor of encoded tokens, where each token is represented as an integer.\n",
        "# the first three elements in the examples are printed\n",
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjtKbgG5Lt4q",
        "outputId": "30ba1231-482b-48ae-d0d0-be84bdbde238"
      },
      "outputs": [],
      "source": [
        "# it shows the original dataset and below it there is After encoded the original dataset which is generated using encoded example\n",
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"After encoded the original dataset: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6sle1vqLv-G"
      },
      "outputs": [],
      "source": [
        "# The Keras API is used in the code to define a deep learning model. The following layers make up the model.\n",
        "model = ten_fw.keras.Sequential([\n",
        "    encoder,\n",
        "# it is used to create dense vector representing of each integer into the sequence of integers product \n",
        "    ten_fw.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "# Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "# It uses the bidirectional LSTM layer that processes the the sequence in both the direction\n",
        "    ten_fw.keras.layers.Bidirectional(ten_fw.keras.layers.LSTM(64)),\n",
        "# the dense hidden layer function is used as relu\n",
        "    ten_fw.keras.layers.Dense(64, activation='relu'),\n",
        "    ten_fw.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFEid-hoL1G6",
        "outputId": "0b02172c-7bb4-476f-b998-dfc9ad6fe81c"
      },
      "outputs": [],
      "source": [
        "# This code snippet prints a list indicating whether each layer in the model supports masking or not. \n",
        "# Masking refers to the process of ignoring certain timesteps in the input sequence during the computation, based on the value of a mask tensor\n",
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18YfDbeCL5km",
        "outputId": "fb11de00-1254-4c09-b45b-2d0fc75f7eca"
      },
      "outputs": [],
      "source": [
        "# here a movie review is written and the model will predict that the sample text review is +ve or -ve\n",
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(npy.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4oSi0t1MAZ1",
        "outputId": "9ead5942-87cd-4bbf-ce5e-103dc1e50e93"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text with padding\n",
        "# The model should output a higher score for the positive sentiment sentence \"sample_text\" compared to the padding sequence.\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(npy.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djK2NukbMBac"
      },
      "outputs": [],
      "source": [
        "#\tloss is the objective that the model tries to minimize during training. \n",
        "# optimizer is the algorithm that updates the weights of the neural network during training in order to minimize the loss function\n",
        "#\tmetrics is a list of metrics used to evaluate the performance of the model during training and testing. \n",
        "# Here, the metric is set to accuracy which is commonly used in binary classification problems to measure the fraction of correctly classified samples\n",
        "model.compile(loss=ten_fw.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=ten_fw.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uibfvF1rMEAe",
        "outputId": "525bb389-9d20-4d60-a029-ff6f7b99dd94"
      },
      "outputs": [],
      "source": [
        "# The fit() method updates the model parameters (i.e., weights) using the algorithm and the optimization algorithm specified in model\n",
        "# During training, it also evaluates the model on the tst_data_set using the validation_data argument, and it computes the validation accuracy at every epoch using the validation_steps argument.\n",
        "history = model.fit(tr_data_set, epochs=10,\n",
        "                    validation_data=tst_data_set,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvxBhVTFqM8O",
        "outputId": "28350068-a319-4e67-a8fd-28dea86241c8"
      },
      "outputs": [],
      "source": [
        "# It uses performance of training dataset on test dataset\n",
        "test_loss, test_acc = model.evaluate(tst_data_set)\n",
        "\n",
        "print('Train Loss:', test_loss)\n",
        "print('Train Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQjcph5rKuzv"
      },
      "outputs": [],
      "source": [
        "# importing matplotlib library for the graph and the graphs will be plotted against Epochs and metric\n",
        "import matplotlib.pyplot as mat_plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  mat_plt.plot(history.history[metric])\n",
        "  mat_plt.plot(history.history['val_'+metric], '')\n",
        "  mat_plt.xlabel(\"Epochs\")\n",
        "  mat_plt.ylabel(metric)\n",
        "  mat_plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "AGLffKjPrZkb",
        "outputId": "c7e2f7e8-77bb-48a3-856e-823c8fad67c0"
      },
      "outputs": [],
      "source": [
        "# The left plot shows the accuracy of the model during training and validation with accuracy as a term\n",
        "# And right plot shows for loss \n",
        "mat_plt.figure(figsize=(16, 8))\n",
        "mat_plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "mat_plt.ylim(None, 1)\n",
        "mat_plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "mat_plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAb9mfQ8tMCR",
        "outputId": "9a106cdb-e00c-4104-9a0d-51212e25cde6"
      },
      "outputs": [],
      "source": [
        "# It generates predictions for the given inpyut text using the trained model\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(npy.array([sample_text]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWcoWWcFtdY_"
      },
      "outputs": [],
      "source": [
        "# encoder - it converts converts text into integer sequences\n",
        "# Embedding - a layer that maps the integer-encoded tokens into dense vectors of fixed size with dimension ogf 64\n",
        "# Activation function is relu \n",
        "# dropout - layer with a rate of 0.5, to randomly drop 50% of the units in the previous layer during training\n",
        "# dense is used for prediction for the sentiment of the inpyut text (positive or negative).\n",
        "model = ten_fw.keras.Sequential([\n",
        "    encoder,\n",
        "    ten_fw.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    ten_fw.keras.layers.Bidirectional(ten_fw.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    ten_fw.keras.layers.Bidirectional(ten_fw.keras.layers.LSTM(32)),\n",
        "    ten_fw.keras.layers.Dense(64, activation='relu'),\n",
        "    ten_fw.keras.layers.Dropout(0.5),\n",
        "    ten_fw.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD5U56MduDWW"
      },
      "outputs": [],
      "source": [
        "# It builds the model using an Adam optimizer with a learning rate of 1e-4, a binary cross-entropy loss function, and accuracy as the evaluation metric.\n",
        "model.compile(loss=ten_fw.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=ten_fw.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N0Z3xpYxG59",
        "outputId": "1391ea0a-8bea-453e-b275-9c18735efc18"
      },
      "outputs": [],
      "source": [
        "# It contain the for code testing the LSTM model defined earlier on the training dataset for 10 epochs, and evaluates the model on the testing dataset after each epoch for 30 validation steps. The training history is stored in the history variable.\n",
        "history = model.fit(tr_data_set, epochs=10,\n",
        "                    validation_data=tst_data_set,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QruNqmkJxOVs"
      },
      "outputs": [],
      "source": [
        " # This block measures the test loss and test accuracy after applying the trained LSTM model to the test dataset.\n",
        "test_loss, test_acc = model.evaluate(tst_data_set)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-cZB2uX8U69"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "# Using the trained LSTM model, the code creates a prediction about the sentiment of the input sample_text.\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(npy.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBYAxQuk8Xvk"
      },
      "outputs": [],
      "source": [
        "# Graphs representing accuracy and loss during training and validation of the LSTM model are plotted using this code.\n",
        "\n",
        "mat_plt.figure(figsize=(16, 6))\n",
        "mat_plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "mat_plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
